spark:
  image: gallna/yarn.spark:2.2.0-4
  # build: .
  # dockerfile: "Dockerfile.spark"
  environment:
    LOG_LEVEL: WARN
    yarn_site: |
      yarn.scheduler.maximum-allocation-mb    4096
      yarn.scheduler.increment-allocation-mb  512
  volumes:
    - ~/projects/python/s3-xml/dist:/usr/local/spark/dist
    # - ./etc/spark/log4j.properties:/usr/local/spark/conf/log4j.properties
    # - ./etc/spark/spark-defaults.conf:/usr/local/spark/conf/spark-defaults.conf
    - ./bin/entrypoint.sh:/usr/local/bin/entrypoint.sh
    # - ./etc/hadoop/yarn-env.sh:/usr/local/hadoop/etc/hadoop/yarn-env.sh
    # - ./etc/hadoop/hadoop-env.sh:/usr/local/hadoop/etc/hadoop/hadoop-env.sh
    # - ./etc/hadoop/log4j.properties:/usr/local/hadoop/etc/hadoop/log4j.properties
    # - ./etc/hadoop/core-site.xml:/usr/local/hadoop/etc/hadoop/core-site.xml
    # - ./etc/hadoop/hdfs-site.xml:/usr/local/hadoop/etc/hadoop/hdfs-site.xml
    # - ./etc/hadoop/yarn-site.xml:/usr/local/hadoop/etc/hadoop/yarn-site.xml
    # - ./etc/hadoop/mapred-site.xml:/usr/local/hadoop/etc/hadoop/mapred-site.xml
    # - ./etc/hadoop/capacity-scheduler.xml:/usr/local/hadoop/etc/hadoop/capacity-scheduler.xml

spark-master:
  extends: spark
  entrypoint: entrypoint.sh --sparkmaster
  container_name: spark-master
  hostname: spark-master
  environment:
    SPARK_MASTER_OPTS: "-Dspark.deploy.defaultCores=1"
  expose:
    - 4040
    - 7077
    - 6066
  ports:
    - 4040:4040
    - 6066:6066
    - 7077:7077

spark-worker:
  extends: spark
  entrypoint: entrypoint.sh -W
  container_name: spark-worker
  hostname: spark-worker
  environment:
    SPARK_WORKER_OPTS:
  expose:
    - 8042
    - 8881

spark-history:
  extends: spark
  entrypoint: entrypoint.sh --spark-history
  container_name: spark-history
  hostname: spark-history
  ports:
    - 18080

resource-manager:
  extends: spark
  entrypoint: entrypoint.sh --resource-manager
  container_name: resource-manager
  hostname: resource-manager
  expose:
    - 8031
    - 8032
    - 8033
  ports:
    - 8088
